{\rtf1\ansi\ansicpg1252\cocoartf1504\cocoasubrtf830
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red38\green38\blue38;}
{\*\expandedcolortbl;;\cssrgb\c20000\c20000\c20000;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\b\fs24 \cf0 \ul \ulc0 Batch sizes\
\
\pard\pardeftab720\partightenfactor0

\b0\fs30 \cf2 \expnd0\expndtw0\kerning0
\ulnone The two main things to consider when optimizing mini-batch size are the time efficiency of training and the noisiness of the gradient estimate. \
\
Let's say we have a dataset with 100,000 training examples, and we are considering a mini-batch size of 100 and 10,000. Computing the gradient of a batch generally involves computing some function over each training example in the batch and summing over the functions. In particular, gradient computation is roughly linear in the batch size. So it's going to take about 100x longer to compute the gradient of a 10,000-batch than a 100-batch. \
\
This means that the 100-batch version of our model is going to make 100 parameter updates in the time it takes the 10,000-batch model to make 1 update. The gradient of the 100-batch isn't going to be quite as accurate as the gradient of a 10,000-batch, so the 100 updates probably won't be 100x as productive as the single update from the larger 10,000-batch. But they might be 10x more productive, which drastically cuts down on total training time.\
\
On to the noisiness part of the equation. When we compute the gradient of a mini-batch, what we're really doing is approximating the gradient of the entire training set. Obviously, the gradient of a single data point is going to be a lot noisier than the gradient of a 100-batch. This means that we won't necessarily be moving down the error function in the direction of steepest descent. \
\
But noisiness isn't all bad. In particular, suppose that our error function is particularly pernicious and has a bunch of little valleys. If we used the entire training set to compute each gradient, our model would get stuck in the first valley it fell into (since it would register a gradient of 0 at this point). If we use smaller mini-batches, on the other hand, we'll get more noise in our estimate of the gradient. This noise might be enough to push us out of some of the shallow valleys in the error function. \
\
There is thus an important trade-off to consider between being able to jump out of shallow minima and making sure that you eventually converge to 
\i some 
\i0 minimum instead of bouncing around. \
\
In practice, small to moderate mini-batches (10-500) are generally used, combined with a decaying learning rate, which guarantees long run convergence while maintaining the power to jump out of shallow minima that are encountered early on.}